{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a15b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,os.path.join(os.getcwd(), 'src')) ## Make ciceroscm importable\n",
    "\n",
    "from ciceroscm import CICEROSCM\n",
    "from ciceroscm.parallel.cscmparwrapper import run_ciceroscm_parallel\n",
    "import ciceroscm.input_handler as input_handler\n",
    "from train import load_processed_data, format_data\n",
    "from model import LSTMSurrogate\n",
    "from data_generation import load_core_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909123a",
   "metadata": {},
   "source": [
    "## 0. Prepare CICERO-SCM for env step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ecc0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.3404531478881836\n",
      "Temperature: 0.7520427266712681\n"
     ]
    }
   ],
   "source": [
    "cfg = [\n",
    "    {\n",
    "        \"pamset_udm\": {\n",
    "            \"rlamdo\": 15.08357,\n",
    "            \"akapa\": 0.6568376339229769,\n",
    "            \"cpi\": 0.2077266,\n",
    "            \"W\": 2.205919,\n",
    "            \"beto\": 6.89822,\n",
    "            \"lambda\": 0.6062529,\n",
    "            \"mixed\": 107.2422,\n",
    "        },\n",
    "        \"pamset_emiconc\": {\n",
    "            \"qbmb\": 0.0,\n",
    "            \"qo3\": 0.5,\n",
    "            \"qdirso2\": -0.3562,\n",
    "            \"qindso2\": -0.96609,\n",
    "            \"qbc\": 0.1566,\n",
    "            \"qoc\": -0.0806,\n",
    "        },\n",
    "        \"Index\": \"13555_old_NR_improved\",\n",
    "    }\n",
    "]\n",
    "\n",
    "conc_data_first = 1750\n",
    "conc_data_last = 2100\n",
    "\n",
    "em_data_start = 1900\n",
    "em_data_policy = 2015\n",
    "em_data_end = 2015\n",
    "\n",
    "gaspam_data, conc_data, em_data, nat_ch4_data, nat_n2o_data = load_core_data()\n",
    "\n",
    "historical_emissions = em_data.loc[:2015]\n",
    "baseline_emissions = em_data.loc[2015+1:]\n",
    "\n",
    "test_data_dir = \"/home/obola/repositories/cicero-scm-surrogate/ciceroscm/tests/test-data\"\n",
    "\n",
    "baseline_scenario = {\n",
    "            \"gaspam_data\": gaspam_data,\n",
    "            \"nyend\": em_data_end,\n",
    "            \"nystart\": em_data_start,\n",
    "            \"emstart\": em_data_policy,\n",
    "            \"concentrations_data\": conc_data,\n",
    "            \"nat_ch4_data\": nat_ch4_data,\n",
    "            \"nat_n2o_data\": nat_n2o_data,\n",
    "            \"emissions_data\": historical_emissions,      \n",
    "            \"udir\": test_data_dir,\n",
    "            \"idtm\": 24,\n",
    "            \"scenname\": \"baseline_scenario\",\n",
    "        }\n",
    "\n",
    "cscm_dir=CICEROSCM(baseline_scenario)\n",
    "t0 = time.time()\n",
    "cscm_dir._run({\"results_as_dict\":True}, pamset_udm=cfg[0][\"pamset_udm\"], pamset_emiconc=cfg[0][\"pamset_emiconc\"])\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"Time: {t1-t0}\")\n",
    "print(f\"Temperature: {cscm_dir.results['dT_glob_air'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01a9182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CICEROSCMEngine:\n",
    "    \"\"\"\n",
    "    Reference engine that consumes full history 1900..t (G=40),\n",
    "    calls CICEROSCM each step, and returns next-year temperature.\n",
    "\n",
    "    Usage:\n",
    "        eng = CICEROSCMEngine(\n",
    "            historical_emissions=historical_emissions_np,\n",
    "            gaspam_data=gaspam_data,\n",
    "            conc_data=conc_data,\n",
    "            nat_ch4_data=nat_ch4_data,\n",
    "            nat_n2o_data=nat_n2o_data,\n",
    "            pamset_udm=cfg[0][\"pamset_udm\"],\n",
    "            pamset_emiconc=cfg[0][\"pamset_emiconc\"],\n",
    "            em_data_start=1900,\n",
    "            em_data_policy=2015,\n",
    "            udir=test_data_dir,  # your output dir\n",
    "            idtm=24,\n",
    "            scenname=\"rl_scenario\",\n",
    "        )\n",
    "        T, info = eng.step(E_t)   # E_t shape (G,)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        historical_emissions,\n",
    "        gaspam_data,\n",
    "        conc_data,\n",
    "        nat_ch4_data,\n",
    "        nat_n2o_data,\n",
    "        pamset_udm,\n",
    "        pamset_emiconc,\n",
    "        em_data_start=1900,\n",
    "        em_data_policy=2015,\n",
    "        udir=\".\",\n",
    "        idtm=24,\n",
    "        scenname=\"rl_scenario\",\n",
    "    ):\n",
    "        # ----- store static inputs -----\n",
    "        self.gaspam_data = gaspam_data\n",
    "        self.conc_data = conc_data\n",
    "        self.nat_ch4_data = nat_ch4_data\n",
    "        self.nat_n2o_data = nat_n2o_data\n",
    "        self.pamset_udm = pamset_udm\n",
    "        self.pamset_emiconc = pamset_emiconc\n",
    "        self.udir = udir\n",
    "        self.idtm = int(idtm)\n",
    "        self.scenname = str(scenname)\n",
    "\n",
    "        self.em_df = historical_emissions\n",
    "        self.current_year = em_data_policy\n",
    "        self.T = 0.0  \n",
    "\n",
    "        # ----- immutable scenario template -----\n",
    "        self._scenario_template = {\n",
    "            \"gaspam_data\": self.gaspam_data,\n",
    "            \"nyend\": self.current_year,\n",
    "            \"nystart\": int(em_data_start),\n",
    "            \"emstart\": int(em_data_policy),\n",
    "            \"concentrations_data\": self.conc_data,\n",
    "            \"nat_ch4_data\": self.nat_ch4_data,\n",
    "            \"nat_n2o_data\": self.nat_n2o_data,\n",
    "            \"emissions_data\": self.em_df,  # will be replaced each step\n",
    "            \"udir\": self.udir,\n",
    "            \"idtm\": self.idtm,\n",
    "            \"scenname\": self.scenname,\n",
    "        }\n",
    "\n",
    "    def _build_scenario(self):\n",
    "        sc = copy.copy(self._scenario_template)\n",
    "        sc[\"emissions_data\"] = self.em_df\n",
    "        sc[\"nyend\"] = int(self.em_df.index[-1])\n",
    "        return sc\n",
    "\n",
    "    def step(self, E_t):\n",
    "        \"\"\"\n",
    "        Append emissions E_t (shape (G,)) for year current_year+1,\n",
    "        run CICEROSCM, and return (T_next, info).\n",
    "        \"\"\"\n",
    "        next_year = int(self.current_year + 1)\n",
    "        e = np.asarray(E_t, dtype=np.float32)\n",
    "\n",
    "        # Append new year to emissions DF\n",
    "        self.em_df = copy.copy(self.em_df)\n",
    "        self.em_df.loc[next_year] = e\n",
    "        self.current_year = next_year\n",
    "\n",
    "        # Build scenario and run SCM (time just the engine)\n",
    "        scenario = self._build_scenario()\n",
    "        cscm = CICEROSCM(scenario)\n",
    "        cscm._run({\"results_as_dict\": True},\n",
    "                            pamset_udm=self.pamset_udm,\n",
    "                            pamset_emiconc=self.pamset_emiconc)\n",
    "\n",
    "        # Extract temperature for next_year\n",
    "        T_next = cscm.results[\"dT_glob_air\"][-1]\n",
    "\n",
    "        self.T = float(T_next)\n",
    "        return self.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e833c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 0\n",
      "Time: 0.39900779724121094\n",
      "Temperature: 0.7945743213966652\n",
      "Year: 1\n",
      "Time: 0.406796932220459\n",
      "Temperature: 0.8315334241383018\n",
      "Year: 2\n",
      "Time: 0.4093959331512451\n",
      "Temperature: 0.8684077734213573\n",
      "Year: 3\n",
      "Time: 0.41803574562072754\n",
      "Temperature: 0.90415658042691\n",
      "Year: 4\n",
      "Time: 0.446674108505249\n",
      "Temperature: 0.9394239810981585\n"
     ]
    }
   ],
   "source": [
    "# Historical emissions matrix H_hist: shape (1900..2015, G)\n",
    "# Keep column order (this must match your per-step E_t order!)\n",
    "scm_engine = CICEROSCMEngine(\n",
    "    historical_emissions=historical_emissions,\n",
    "    gaspam_data=gaspam_data,\n",
    "    conc_data=conc_data,\n",
    "    nat_ch4_data=nat_ch4_data,\n",
    "    nat_n2o_data=nat_n2o_data,\n",
    "    pamset_udm=cfg[0][\"pamset_udm\"],\n",
    "    pamset_emiconc=cfg[0][\"pamset_emiconc\"],\n",
    "    em_data_start=em_data_start,\n",
    "    em_data_policy=em_data_policy,\n",
    "    udir=test_data_dir,\n",
    "    idtm=24,\n",
    "    scenname=\"scm_unit_test\",\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Year: {i}\")\n",
    "    next_emission = copy.copy(baseline_emissions.iloc[i].values)\n",
    "    t0 = time.time()\n",
    "    T = scm_engine.step(next_emission)\n",
    "    t1 = time.time()\n",
    "    print(f\"Time: {t1-t0}\")\n",
    "    print(f\"Temperature: {T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1b796",
   "metadata": {},
   "source": [
    "## 1. Prepare CICERO-NET as env step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "126791b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "run_dir=\"/home/obola/repositories/cicero-scm-surrogate/data/20250805_152136\"\n",
    "device=\"cuda:0\"\n",
    "weights_name=\"model_lstm.pth\"\n",
    "\n",
    "model = LSTMSurrogate(n_gas=40, hidden=128, num_layers=1).to(device)\n",
    "wpath = os.path.join(run_dir, weights_name)\n",
    "state = torch.load(wpath, map_location=device, weights_only=False)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "cicero_net = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14bcb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standardizer\n",
    "run_path = \"/home/obola/repositories/cicero-scm-surrogate/data/20250805_152136\"\n",
    "data_path = os.path.join(run_path, \"processed\")\n",
    "\n",
    "X_train = np.load(os.path.join(data_path, \"X_train.npy\"))\n",
    "\n",
    "gas_mu  = X_train.reshape(-1, 40).mean(axis=0)     # per-gas mean\n",
    "gas_std = X_train.reshape(-1, 40).std(axis=0) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e8c1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CICERONetEngine:\n",
    "    \"\"\"\n",
    "    Inference engine for a CICERO-NET model trained on inputs shaped (51, G):\n",
    "      [50 past years; current year's emissions], target = T_{t+1}.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    historical_emissions : np.ndarray, shape (T_hist, G)\n",
    "        Full history up to and including 2015. We keep the last 50 rows.\n",
    "    model : torch.nn.Module\n",
    "        Trained surrogate (expects (B, 51, G) and returns (B, 1) or (B,)).\n",
    "    device : str\n",
    "        \"cuda:0\" or \"cpu\".\n",
    "    window : int\n",
    "        Number of historical years (should be 50 to match training).\n",
    "    mu, std : np.ndarray or torch.Tensor, shape (G,), optional\n",
    "        Per-gas normalization used at training for the 51×G inputs.\n",
    "        If None, no normalization is applied.\n",
    "    autocast : bool\n",
    "        Enable mixed precision (only meaningful on CUDA when use_half=True).\n",
    "    use_half : bool\n",
    "        Use float16 inside autocast; otherwise run in float32.\n",
    "    \"\"\"\n",
    "    def __init__(self, historical_emissions, model, device=\"cuda:0\", window=50,\n",
    "                 mu=None, std=None, autocast=True, use_half=True):\n",
    "        import torch\n",
    "        self.W = int(window)\n",
    "        self.device = torch.device(device)\n",
    "        self.model = model.eval().to(self.device)\n",
    "        self.use_half = bool(use_half) and (self.device.type == \"cuda\")\n",
    "        # Only enable autocast if we’re actually using half precision.\n",
    "        self.autocast = bool(autocast) and self.use_half\n",
    "\n",
    "        # Fast kernels (safe no-ops on CPU)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision(\"high\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ---- keep exactly the last 50 rows (t-50..t-1) as the rolling buffer ----\n",
    "        hist_tail = np.asarray(historical_emissions[-self.W:], dtype=np.float32)   # (50, G)\n",
    "        self.G = hist_tail.shape[1]\n",
    "        self.buf = torch.from_numpy(hist_tail).to(self.device, non_blocking=True)  # (50, G)\n",
    "\n",
    "        # Model input is (1, 51, G)\n",
    "        dtype = torch.float16 if self.use_half else torch.float32\n",
    "        self.x = torch.empty((1, self.W + 1, self.G), device=self.device, dtype=dtype)\n",
    "\n",
    "        # Optional per-gas normalization stats (on device, dtype-matched)\n",
    "        self.mu  = None if mu  is None else torch.as_tensor(mu,  device=self.device, dtype=self.x.dtype)\n",
    "        self.std = None if std is None else torch.as_tensor(std, device=self.device, dtype=self.x.dtype)\n",
    "\n",
    "        self.T = 0.0  # last predicted temperature (scalar float)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def step(self, E_t):\n",
    "        \"\"\"\n",
    "        Feed [buf(50,G); E_t(1,G)] → model → T_{t+1}; then roll buf to include E_t.\n",
    "        E_t : array-like, shape (G,)\n",
    "        Returns:\n",
    "            float temperature prediction for next year.\n",
    "        \"\"\"\n",
    "        e = torch.as_tensor(E_t, device=self.device, dtype=self.buf.dtype)  # (G,)\n",
    "\n",
    "        # Build (51, G) input: first the 50 past rows …\n",
    "        self.x[0, :self.W].copy_(self.buf, non_blocking=True)\n",
    "        # … then append current action-year emissions\n",
    "        self.x[0, self.W].copy_(e, non_blocking=True)\n",
    "\n",
    "        # Normalize inputs if stats provided: (x - mu)/std (broadcast over time)\n",
    "        if (self.mu is not None) and (self.std is not None):\n",
    "            self.x[0].sub_(self.mu).div_(self.std)\n",
    "\n",
    "        # Forward pass\n",
    "        if self.autocast:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                out = self.model(self.x)\n",
    "        else:\n",
    "            out = self.model(self.x)\n",
    "\n",
    "        T_next = float(out.squeeze().item())\n",
    "        self.T = T_next\n",
    "\n",
    "        # Update the rolling 50-year buffer for the next call: drop oldest, append E_t\n",
    "        self.buf = torch.roll(self.buf, shifts=-1, dims=0)\n",
    "        self.buf[-1].copy_(e, non_blocking=True)\n",
    "\n",
    "        return self.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6d4bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 0\n",
      "Time: 0.0014331340789794922\n",
      "Temperature: 0.7944691777229309\n",
      "Year: 1\n",
      "Time: 0.0009052753448486328\n",
      "Temperature: 0.8315608501434326\n",
      "Year: 2\n",
      "Time: 0.0008723735809326172\n",
      "Temperature: 0.8684326410293579\n",
      "Year: 3\n",
      "Time: 0.0008866786956787109\n",
      "Temperature: 0.9042533040046692\n",
      "Year: 4\n",
      "Time: 0.0008101463317871094\n",
      "Temperature: 0.9391978979110718\n"
     ]
    }
   ],
   "source": [
    "net_engine = CICERONetEngine(historical_emissions = historical_emissions,\n",
    "                             model = cicero_net,\n",
    "                             device=\"cuda:0\", \n",
    "                             window=50,\n",
    "                             mu=gas_mu, \n",
    "                             std=gas_std, \n",
    "                             autocast=True, \n",
    "                             use_half=False)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Year: {i}\")\n",
    "    next_emission = copy.copy(baseline_emissions.iloc[i].values)\n",
    "    t0 = time.time()\n",
    "    T = net_engine.step(next_emission)\n",
    "    t1 = time.time()\n",
    "    print(f\"Time: {t1-t0}\")\n",
    "    print(f\"Temperature: {T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723f594",
   "metadata": {},
   "source": [
    "## 2. MARL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15b599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8382eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf552652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b82af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c41050",
   "metadata": {},
   "source": [
    "## 3. OLD MARL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b60e281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateMARL(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    N agents (countries). Each year they pick a deviation a_i from their baseline share.\n",
    "    Total emissions E_t (40 gases) feed a climate engine (CICERO-NET or CICERO-SCM).\n",
    "    Observation (same for all agents): [T_t, year_norm, B_t(40)].\n",
    "    Reward_i = xxx\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config, emission_data, economics_config, actions_config):\n",
    "        super().__init__()\n",
    "        # --- config dicts ---\n",
    "        rng = np.random.default_rng(env_config['random_seed'])\n",
    "      \n",
    "        # --- core sizes/time ---\n",
    "        self.N = int(env_config[\"N\"])\n",
    "        self.G = int(env_config[\"G\"])\n",
    "        self.year0_hist = int(env_config[\"year0_hist\"])\n",
    "        self.hist_end   = int(env_config[\"hist_end\"])\n",
    "        self.future_end = int(env_config[\"future_end\"])\n",
    "        self.horizon    = int(env_config[\"horizon\"])\n",
    "        self.engine_kind = str(env_config['engine']).lower()\n",
    "        \n",
    "        # --- provided data ---\n",
    "        self.historical_emissions  = np.asarray(emission_data['historical_emissions'], dtype=np.float32)\n",
    "        self.baseline_emissions    = np.asarray(emission_data[\"baseline_emissions\"], dtype=np.float32)\n",
    "        self.emission_shares       = np.asarray(emission_data[\"emission_shares\"], dtype=np.float32)\n",
    "\n",
    "        # --- heterogeneous economics/impacts ---\n",
    "        self.climate_disaster_cost         = np.asarray(economics_config['climate_disaster_cost'], dtype=np.float32)         # (N,)\n",
    "        self.climate_investment_cost       = np.asarray(economics_config['climate_investment_cost'], dtype=np.float32)       # (N,)\n",
    "        self.economic_growth_sensitivity   = np.asarray(economics_config['economic_growth_sensitivity'], dtype=np.float32)   # (N,)\n",
    "\n",
    "        # --- actions ---\n",
    "        self.actions = np.asarray(actions_config['m_levels'], dtype=np.float32)  # e.g. [-0.05, ..., 0.05]\n",
    "        self._act_space = spaces.Discrete(len(self.actions))  # <-- use actions\n",
    "\n",
    "        # --- agents  ---\n",
    "        self.agents = [f\"country_{i}\" for i in range(self.N)]\n",
    "\n",
    "        # --- spaces ---\n",
    "        obs_low  = np.array([0, 0, ] + [0.0]*self.G, dtype=np.float32)\n",
    "        obs_high = np.array([2.0, 35,] + [10000]*self.G, dtype=np.float32)\n",
    "        self._obs_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "        self.observation_spaces = {a: self._obs_space for a in self.agents}\n",
    "        self.action_spaces      = {a: self._act_space for a in self.agents}\n",
    "\n",
    "        # --- model parameters ---\n",
    "        self.net_params = env_config.get(\"net_params\", {})\n",
    "        self.scm_params = env_config.get(\"scm_params\", {})\n",
    "\n",
    "        # internal state\n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    def observation_space(self, agent_id=None):\n",
    "        return self.observation_spaces[self.agents[0] if agent_id is None else agent_id]\n",
    "\n",
    "    def action_space(self, agent_id=None):\n",
    "        return self.action_spaces[self.agents[0] if agent_id is None else agent_id]\n",
    "\n",
    "    def _make_engine(self):\n",
    "        if self.engine_kind == \"net\":\n",
    "            p = self.net_params\n",
    "            # rebuild model from class path\n",
    "            module_name, cls_name = p[\"model_class_path\"].rsplit(\".\", 1)\n",
    "            Mod = __import__(module_name, fromlist=[cls_name])\n",
    "            ModelCls = getattr(Mod, cls_name)\n",
    "            m = ModelCls(**p[\"model_kwargs\"])\n",
    "            m.load_state_dict(p[\"state_dict\"])\n",
    "            # construct the high-throughput engine\n",
    "            return CICERONetEngine(\n",
    "                historical_emissions=self.historical_emissions,\n",
    "                model=m,\n",
    "                device=p.get(\"device\", \"cpu\"),\n",
    "                window=int(p.get(\"window\", 50)),\n",
    "                mu=np.asarray(p[\"mu\"], np.float32) if p.get(\"mu\") is not None else None,\n",
    "                std=np.asarray(p[\"std\"], np.float32) if p.get(\"std\") is not None else None,\n",
    "                autocast=bool(p.get(\"autocast\", False)),\n",
    "                use_half=bool(p.get(\"use_half\", False)),\n",
    "            )\n",
    "        elif self.engine_kind == \"scm\":\n",
    "            p = self.net_params\n",
    "            \n",
    "            return CICEROSCMEngine(\n",
    "                historical_emissions=self.historical_emissions,\n",
    "                gaspam_data=p[\"gaspam_data\"],\n",
    "                conc_data=p[\"conc_data\"],\n",
    "                nat_ch4_data=p[\"nat_ch4_data\"],\n",
    "                nat_n2o_data=p[\"nat_n2o_data\"],\n",
    "                pamset_udm=p[\"pamset_udm\"],\n",
    "                pamset_emiconc=p[\"pamset_emiconc\"],\n",
    "                em_data_start=p[\"em_data_start\"],\n",
    "                em_data_policy=p[\"em_data_policy\"],\n",
    "                udir=p[\"udir\"],\n",
    "                idtm=p[\"idtm\"],\n",
    "                scenname=p[\"scenname\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(self.engine_kind)\n",
    "\n",
    "    def _obs(self):\n",
    "        temp = float(self.engine.T)\n",
    "        year_idx = float(self.year_idx - (self.hist_end + 1))  # 0..35\n",
    "        emissions = self.historical_emissions[-1]              # (G,)\n",
    "        return np.asarray([temp, year_idx, *emissions], dtype=np.float32)\n",
    "\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            _ = np.random.default_rng(seed)\n",
    "        self.t = 0\n",
    "        self.year_idx = self.hist_end + 1  # 2016\n",
    "        self.engine = self._make_engine()\n",
    "        obs = self._obs()\n",
    "        return {a: obs for a in self.agents}, {a: {} for a in self.agents}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        \n",
    "        a_vec = np.array([self.actions[action_dict[ag]] for ag in self.agents], dtype=np.float32)  # (N,)\n",
    "        \n",
    "        B_t = self.baseline_emissions[self.year_idx - (self.hist_end + 1)]  # (40,)\n",
    "        \n",
    "        b_alloc = self.emission_shares * B_t[None, :]  # (N,40)\n",
    "        \n",
    "        e_agents = (1.0 + a_vec[:, None]) * b_alloc\n",
    "        e_agents = np.clip(e_agents, 0.0, None)\n",
    "        \n",
    "        E_t = e_agents.sum(axis=0)  # (40,)\n",
    "        \n",
    "        self.historical_emissions = np.vstack([self.historical_emissions, E_t[None, :]])\n",
    "        T_next = self.engine.step(E_t)\n",
    "\n",
    "        # 6) rewards\n",
    "        emiss_agent_sum = e_agents.sum(axis=1)   # (N,)\n",
    "        base_agent_sum  = b_alloc.sum(axis=1)    # (N,)\n",
    "        delta_emissions = a_vec * base_agent_sum   # (N,)\n",
    "\n",
    "        # (A) Climate disasters cost: penalize high temperature (use T_t, i.e., pre-update)\n",
    "        disaster_cost = self.climate_disaster_cost * (T_next ** 2)   # (N,)\n",
    "\n",
    "        # (B) Climate investment cost: only when mitigating (a < 0).\n",
    "        # Scale by the size of the baseline so deeper cuts cost more.\n",
    "        mitigation_amount = np.clip(-a_vec, 0.0, None)                      # (-a) if a<0 else 0\n",
    "        investment_cost  = self.climate_investment_cost * mitigation_amount * base_agent_sum  # (N,)\n",
    "\n",
    "        # (C) Economic growth sensitivity: more emissions boosts growth (a>0), cuts reduce it (a<0).\n",
    "        # Linear in delta emissions; you can make it nonlinear later if you want.\n",
    "        growth_term = self.economic_growth_sensitivity * delta_emissions    # (N,)\n",
    "\n",
    "        r = growth_term - investment_cost - disaster_cost                   # (N,)\n",
    "        \n",
    "        self.t += 1\n",
    "        self.year_idx += 1\n",
    "        done = (self.t >= self.horizon) or (self.year_idx > self.future_end)\n",
    "        \n",
    "        obs = self._obs()\n",
    "        obs_d = {a: obs for a in self.agents}\n",
    "        \n",
    "        rew_d   = {a: float(x) for a, x in zip(self.agents, r)}\n",
    "        term_d  = {a: done for a in self.agents}\n",
    "        trunc_d = {a: False for a in self.agents}\n",
    "        info_d  = {a: {\"E_sum\": float(E_t.sum())} for a in self.agents}\n",
    "        term_d[\"__all__\"]  = done\n",
    "        trunc_d[\"__all__\"] = False\n",
    "\n",
    "        return obs_d, rew_d, term_d, trunc_d, info_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37322d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obola/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/obola/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/obola/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/obola/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 108\u001b[0m\n\u001b[1;32m     86\u001b[0m algo \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     87\u001b[0m     PPOConfig()\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;241m.\u001b[39menvironment(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m--> 108\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     mean_ret \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    110\u001b[0m         result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_return_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_runners\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_return_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: mean_return=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_ret\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmean_ret\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn/a\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:331\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m skipped \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:1040\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration()\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         (\n\u001b[1;32m   1038\u001b[0m             train_results,\n\u001b[1;32m   1039\u001b[0m             train_iter_ctx,\n\u001b[0;32m-> 1040\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:4098\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration_old_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4091\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_on_env_runners_recreated_callbacks(\n\u001b[1;32m   4092\u001b[0m             config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   4093\u001b[0m             env_runner_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group,\n\u001b[1;32m   4094\u001b[0m             restored_env_runner_indices\u001b[38;5;241m=\u001b[39mrestored,\n\u001b[1;32m   4095\u001b[0m         )\n\u001b[1;32m   4097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 4098\u001b[0m     training_step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_results:\n\u001b[1;32m   4101\u001b[0m     results \u001b[38;5;241m=\u001b[39m training_step_results\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py:391\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@override\u001b[39m(Algorithm)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# Old API stack (Policy, RolloutWorker, Connector).\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# Collect batches from sample workers until we have a full batch.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, ENV_RUNNER_SAMPLING_TIMER)):\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;66;03m# Sample in parallel from the workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py:470\u001b[0m, in \u001b[0;36mPPO._training_step_old_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    465\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group,\n\u001b[1;32m    466\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[1;32m    467\u001b[0m         sample_timeout_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msample_timeout_s,\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runner_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Return early if all our workers failed.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_batch:\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/execution/rollout_ops.py:101\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, random_actions, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m<\u001b[39m max_agent_or_env_steps\n\u001b[1;32m     97\u001b[0m ):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# samples.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m         sampled_data \u001b[38;5;241m=\u001b[39m [\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _return_metrics:\n\u001b[1;32m    103\u001b[0m             stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_env_runner\u001b[38;5;241m.\u001b[39mget_metrics()]\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:677\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_start\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    671\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    672\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample batch of size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    673\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_rollout_fragment_length\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m     )\n\u001b[0;32m--> 677\u001b[0m batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    678\u001b[0m steps_so_far \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    679\u001b[0m     batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcount\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcount_steps_by \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magent_steps()\n\u001b[1;32m    682\u001b[0m )\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py:59\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;129m@override\u001b[39m(InputReader)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[0;32m---> 59\u001b[0m     batches \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     60\u001b[0m     batches\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extra_batches())\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/evaluation/sampler.py:225\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@override\u001b[39m(SamplerInput)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SampleBatchType:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_runner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_queue\u001b[38;5;241m.\u001b[39mput(item)\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py:329\u001b[0m, in \u001b[0;36mEnvRunnerV2.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Samples and yields training episodes continuously.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    Object containing state, action, reward, terminal condition,\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    and other fields as dictated by `policy`.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/evaluation/env_runner_v2.py:385\u001b[0m, in \u001b[0;36mEnvRunnerV2.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Return computed actions to ready envs. We also send to envs that have\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# taken off-policy actions; those envs are free to ignore the action.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m t4 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions_to_send\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perf_stats\u001b[38;5;241m.\u001b[39mincr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_wait_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, env_poll_time \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t4)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_render()\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py:553\u001b[0m, in \u001b[0;36mMultiAgentEnvWrapper.send_actions\u001b[0;34m(self, action_dict)\u001b[0m\n\u001b[1;32m    551\u001b[0m         infos \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    556\u001b[0m     obs, (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m    557\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot a multi-agent obs dict or an Exception!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rewards, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot a multi-agent reward dict!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cicero/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env.py:542\u001b[0m, in \u001b[0;36mMultiAgentEnvWrapper.send_actions\u001b[0;34m(self, action_dict)\u001b[0m\n\u001b[1;32m    540\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_id]\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     obs, rewards, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestart_failed_sub_environments:\n",
      "Cell \u001b[0;32mIn[52], line 131\u001b[0m, in \u001b[0;36mClimateMARL.step\u001b[0;34m(self, action_dict)\u001b[0m\n\u001b[1;32m    128\u001b[0m E_t \u001b[38;5;241m=\u001b[39m e_agents\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (40,)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistorical_emissions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistorical_emissions, E_t[\u001b[38;5;28;01mNone\u001b[39;00m, :]])\n\u001b[0;32m--> 131\u001b[0m T_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# 6) rewards\u001b[39;00m\n\u001b[1;32m    134\u001b[0m emiss_agent_sum \u001b[38;5;241m=\u001b[39m e_agents\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# (N,)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 84\u001b[0m, in \u001b[0;36mCICEROSCMEngine.step\u001b[0;34m(self, E_t)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Append new year to emissions DF\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem_df \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem_df)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mem_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[next_year] \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_year \u001b[38;5;241m=\u001b[39m next_year\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Build scenario and run SCM (time just the engine)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "# ----------------------------- Training with RLlib -----------------------------\n",
    "# Example configuration\n",
    "env_config = {\"N\": 4, \n",
    "              \"engine\": \"scm\", \n",
    "              \"horizon\": 35, \n",
    "              \"G\": 40, \n",
    "              \"year0_hist\": 1900, \n",
    "              \"hist_end\": 2015, \n",
    "              \"future_end\": 2050,\n",
    "              \"random_seed\": 0,\n",
    "              }\n",
    "\n",
    "if env_config['engine'] == \"net\":\n",
    "    # make a small, CPU state_dict to put in env_config\n",
    "    state_dict_cpu = {k: v.cpu() for k, v in cicero_net.state_dict().items()}\n",
    "    net_params = {\n",
    "        \"model_class_path\": \"model.LSTMSurrogate\",\n",
    "        \"model_kwargs\": {\"n_gas\": 40, \"hidden\": 128, \"num_layers\": 1},\n",
    "        \"state_dict\": state_dict_cpu,     # <- picklable\n",
    "        \"device\": \"cuda:0\",\n",
    "        \"window\": 50,\n",
    "        \"mu\": gas_mu.tolist() if gas_mu is not None else None,\n",
    "        \"std\": gas_std.tolist() if gas_std is not None else None,\n",
    "        \"autocast\": True,\n",
    "        \"use_half\": False,\n",
    "    }\n",
    "    env_config[\"net_params\"] = net_params\n",
    "\n",
    "elif env_config['engine'] == \"scm\":\n",
    "    net_params = {\n",
    "        \"gaspam_data\": gaspam_data,\n",
    "        \"conc_data\": conc_data,\n",
    "        \"nat_ch4_data\": nat_ch4_data,\n",
    "        \"nat_n2o_data\": nat_n2o_data,\n",
    "        \"pamset_udm\": cfg[0][\"pamset_udm\"],\n",
    "        \"pamset_emiconc\": cfg[0][\"pamset_emiconc\"],\n",
    "        \"em_data_start\": em_data_start,\n",
    "        \"em_data_policy\": em_data_policy,\n",
    "        \"udir\": test_data_dir,\n",
    "        \"idtm\": 24,\n",
    "        \"scenname\": \"scm_marl_test\"\n",
    "    }\n",
    "    env_config[\"net_params\"] = net_params\n",
    "\n",
    "\n",
    "# Example \n",
    "emission_shares = np.ones((4, 40), dtype=np.float32) / 4.0  # replace with real data\n",
    "\n",
    "emission_data = {\n",
    "    \"historical_emissions\": historical_emissions,\n",
    "    \"baseline_emissions\": baseline_emissions,\n",
    "    \"emission_shares\": emission_shares}\n",
    "\n",
    "# Economic parameters per country\n",
    "economics_config = {\n",
    "    \"climate_disaster_cost\":       [0.5, 0.7, 0.9, 0.85],  # cost of climate change per temperature increase (per country)\n",
    "    \"climate_investment_cost\":     [0.5, 0.7, 0.9, 0.85],  # cost of climate investment\n",
    "    \"economic_growth_sensitivity\": [0.5, 0.7, 0.9, 0.85],  # economic growth sensitivity to investment in prevention\n",
    "    }\n",
    "\n",
    "actions_config = {\n",
    "    \"m_levels\": [-0.05, -0.03, -0.01, 0.0, 0.01, 0.03, 0.05]  # Deviation from baseline emission shares\n",
    "    }\n",
    "\n",
    "# Build a temp env to read spaces and N\n",
    "tmp = ClimateMARL(env_config, emission_data, economics_config, actions_config)\n",
    "obs_sp = tmp.observation_space(\"country_0\")\n",
    "act_sp = tmp.action_space(\"country_0\")\n",
    "N = env_config[\"N\"]\n",
    "\n",
    "# One policy per country (no sharing)\n",
    "policies = {f\"country_{i}\": (None, obs_sp, act_sp, {}) for i in range(N)}\n",
    "policy_mapping_fn = lambda agent_id, *_, **__: agent_id\n",
    "\n",
    "def env_creator(cfg):\n",
    "    # cfg is the single env_config RLlib passes.\n",
    "    return ClimateMARL(\n",
    "        cfg[\"env_config\"],\n",
    "        cfg[\"emission_data\"],\n",
    "        cfg[\"economics_config\"],\n",
    "        cfg[\"actions_config\"],\n",
    "    )\n",
    "\n",
    "register_env(\"climate_marl\", env_creator)\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"climate_marl\",\n",
    "        env_config={\n",
    "            \"env_config\": env_config,\n",
    "            \"emission_data\": emission_data,\n",
    "            \"economics_config\": economics_config,\n",
    "            \"actions_config\": actions_config,\n",
    "        },\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    # ↓↓↓ switch to old stack so num_gpus works as expected\n",
    "    .api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)\n",
    "    .env_runners(num_env_runners=0, num_envs_per_env_runner=1)\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]}, gamma=0.99, lr=2e-3, train_batch_size=512)\n",
    "    .multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
    "    .resources(num_gpus=1)     \n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    result = algo.train()\n",
    "    mean_ret = (\n",
    "        result.get(\"episode_reward_mean\")\n",
    "        or result.get(\"episode_return_mean\")\n",
    "        or result.get(\"env_runners\", {}).get(\"episode_return_mean\")\n",
    "        or result.get(\"sampler_results\", {}).get(\"episode_reward_mean\")\n",
    "    )\n",
    "    print(f\"iter {i}: mean_return={mean_ret if mean_ret is not None else 'n/a'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e773c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cicero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
